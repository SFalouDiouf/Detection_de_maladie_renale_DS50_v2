# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 02_Pretraitement.py â€“ pipeline robuste + UI explicative
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import streamlit as st
import pandas as pd, numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONFIG STREAMLIT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
st.set_page_config(page_title="ğŸ§¼ PrÃ©-traitement", page_icon="ğŸ§¹", layout="wide")
st.title("ğŸ§¹ Pipeline de PrÃ©-traitement des DonnÃ©es")

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CHARGEMENT DF â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
if "uploaded_df" not in st.session_state:
    st.warning("Importez dâ€™abord le CSV dans la page Â« Exploration Â». ğŸš©")
    st.stop()

df_raw = st.session_state.uploaded_df.copy()
if "raw_df" not in st.session_state:
    st.session_state["raw_df"] = df_raw.copy()
if "id" in df_raw.columns:
    df_raw.drop(columns=["id"], inplace=True)

df_raw["classification"] = (
    df_raw["classification"].astype(str).str.strip().str.lower()
)

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TRANSFORMER : strip "," â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
class StripThousands(BaseEstimator, TransformerMixin):
    """Supprime la virgule des milliers et convertit en float."""
    def fit(self, X, y=None): return self
    def transform(self, X):
        return pd.DataFrame(
            X.astype(str).str.replace(",", "", regex=False).astype(float),
            columns=X.columns,
            index=X.index,
        )

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ EXPANDER 1 : DIAGNOSTIC NA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
with st.expander("ğŸ” Ã‰tape 1 â€“ Diagnostic des valeurs manquantes", expanded=True):
    miss = (
        df_raw.isna().sum()
        .loc[lambda s: s > 0]
        .to_frame("Total NA")
        .assign(**{"% NA": lambda d: (d["Total NA"] / len(df_raw) * 100).round(1)})
        .sort_values("% NA", ascending=False)
    )
    if miss.empty:
        st.success("âœ… Aucune valeur manquante.")
    else:
        st.dataframe(miss)
        st.markdown(
            "*RÃ¨gle :* â‰¤ 5 % â‡’ imputation simple â€¢ > 5 % â‡’ imputation KNN (num) "
            "ou catÃ©gorie Â« missing Â» (cat)."
        )

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FONCTION Dâ€™IMPUTATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
def impute_df(raw: pd.DataFrame) -> pd.DataFrame:
    df = raw.copy()

    # 1) â€œ?â€ â†’ NaN
    df.replace("?", np.nan, inplace=True)

    # 2) Conversion des colonnes numÃ©riques (virgules milliers)
    if "wc" in df.columns:
        df["wc"] = df["wc"].astype(str).str.replace(",", "").replace("nan", np.nan)

    numeric_force = ["age", "bp", "bgr", "bu", "sc", "sod", "pot",
                     "hemo", "pcv", "wc", "rc"]
    for col in numeric_force:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")

    # 3) Binarisation explicite
    mapping = {"yes": 1, "no": 0,
               "present": 1, "notpresent": 0,
               "abnormal": 1, "normal": 0,
               "good": 1, "poor": 0}
    bin_cols = ["rbc", "pc", "pcc", "ba", "htn", "dm",
                "cad", "appet", "pe", "ane"]
    for col in bin_cols:
        if col in df.columns:
            df[col] = (df[col].astype(str)
                              .str.strip()
                              .str.lower()
                              .map(mapping)
                              .replace("nan", np.nan))

    # 4) Imputation
    na_pct = df.isna().mean() * 100
    low_na  = [c for c in df.columns if na_pct[c] <= 5]
    high_na = [c for c in df.columns if na_pct[c] > 5]

    for c in low_na:
        if df[c].dtype.kind in "fi":
            df[c].fillna(df[c].median(), inplace=True)
        else:
            df[c].fillna(df[c].mode().iloc[0], inplace=True)

    num_high = [c for c in high_na if df[c].dtype.kind in "fi"]
    cat_high = [c for c in high_na if c not in num_high]

    if num_high:
        df[num_high] = KNNImputer(n_neighbors=3).fit_transform(df[num_high])
    for c in cat_high:
        df[c] = df[c].fillna("missing")

    # 5) Cible binaire
    df["classification"] = df["classification"].replace(
        {"ckd": 1, "ckd\t": 1, "notckd": 0}
    )
    return df

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ EXPANDER 2 : IMPUTATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
with st.expander("ğŸ› ï¸ Ã‰tape 2 â€“ Nettoyage & imputation", expanded=False):
    st.markdown(
        """
        *Actions :*  
        1. Normaliser les valeurs binaires, supprimer les virgules milliers  
        2. Appliquer la stratÃ©gie dâ€™imputation dÃ©finie Ã  lâ€™Ã©tape 1  
        """
    )
    if st.button("ğŸ” Lancer lâ€™imputation"):
        df_imp = impute_df(df_raw)
        st.session_state.df_imputed = df_imp
        st.success("Imputation terminÃ©e âœ…")
        st.dataframe(df_imp.head())

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ EXPANDER 3 : PIPELINE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
with st.expander("âš™ï¸ Ã‰tape 3 â€“ Construction du pipeline Scaler + OHE", expanded=False):
    if "df_imputed" not in st.session_state:
        st.info("Veuillez dâ€™abord lancer lâ€™imputation.")
    else:
        df_imp = st.session_state.df_imputed
        num_cols = df_imp.select_dtypes(include="number").columns.tolist()
        cat_cols = (df_imp.select_dtypes(exclude="number")
                          .drop(columns=["classification"], errors="ignore")
                          .columns.tolist())

        num_pipe = Pipeline([
            ("strip",   StripThousands()),
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler",  StandardScaler())
        ])
        try:
            ohe = OneHotEncoder(handle_unknown="ignore", sparse_output=False)
        except TypeError:
            ohe = OneHotEncoder(handle_unknown="ignore", sparse=False)

        prep_pipeline = ColumnTransformer([
            ("num", num_pipe, num_cols),
            ("cat", ohe,      cat_cols)
        ])
        st.session_state.prep_pipeline = prep_pipeline
        st.success("ğŸ“¦ prep_pipeline enregistrÃ© dans la session.")
        st.markdown(
            "*Pourquoi ?* â†’ garantir un prÃ©-traitement identique en "
            "validation croisÃ©e **et** en production."
        )

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ EXPANDER 4 : RÃ‰CAP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
with st.expander("âœ… Ã‰tape 4 â€“ RÃ©capitulatif & stockage", expanded=False):
    if "df_imputed" in st.session_state:
        df_imp = st.session_state.df_imputed
        col1, col2, col3 = st.columns(3)
        col1.metric("Lignes", df_imp.shape[0])
        col2.metric("Colonnes", df_imp.shape[1])
        col3.metric("NA restantes", int(df_imp.isna().sum().sum()))
        st.dataframe(df_imp.head())

        # Stockage pour les autres pages
        st.session_state.cleaned_df = df_imp

        # â”€â”€â”€ Bouton de tÃ©lÃ©chargement â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        csv = df_imp.to_csv(index=False).encode("utf-8")
        st.download_button(
            label="ğŸ“¥ TÃ©lÃ©charger les donnÃ©es nettoyÃ©es",
            data=csv,
            file_name="donnees_nettoyees.csv",
            mime="text/csv"
        )

        st.success(
            "Les donnÃ©es nettoyÃ©es **et** le pipeline sont prÃªts pour la page "
            "Â« ModÃ©lisation Â». Tout est conservÃ© en mÃ©moire."
        )
    else:
        st.info("Terminez les Ã©tapes prÃ©cÃ©dentes pour accÃ©der au rÃ©sumÃ©.")
